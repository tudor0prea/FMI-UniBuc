{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acc7d14",
   "metadata": {},
   "source": [
    "## Retele de perceptroni - Pytorch & Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e013ba9",
   "metadata": {},
   "source": [
    "### Definirea unei retele de perceptroni in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7e36b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier # importul clasei\n",
    "\n",
    "\n",
    "mlp_classifier_model = MLPClassifier(hidden_layer_sizes=(100, ),\n",
    "activation='relu', solver='adam', alpha=0.0001, batch_size='auto',\n",
    "learning_rate='constant', learning_rate_init=0.001, power_t=0.5,\n",
    "max_iter=200, shuffle=True, random_state=None, tol=0.0001,\n",
    "momentum=0.9, early_stopping=False, validation_fraction=0.1,\n",
    "n_iter_no_change=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399b9c5",
   "metadata": {},
   "source": [
    "Parametrii:\n",
    "- hidden_layer_sizes (tuple, lungime= n_layers - 2, default=(100,)): al i-lea\n",
    "element reprezinta numarul de neurori din al i-lea strat ascuns.\n",
    "- activation( {â€˜identityâ€™, â€˜logisticâ€™, â€˜tanhâ€™, â€˜reluâ€™}, default=â€˜reluâ€™)\n",
    "- â€˜Identityâ€™: ğ‘“(ğ‘¥) = ğ‘¥\n",
    "- â€˜logisticâ€™ : ğ‘“(ğ‘¥) = 1\n",
    "1 + Ïµâˆ’ğ‘¥\n",
    "- â€˜tanhâ€™ : ğ‘“(ğ‘¥) = ğ‘¡ğ‘ğ‘›â„(ğ‘¥)\n",
    "- â€˜reluâ€™ : ğ‘“(ğ‘¥) = ğ‘šğ‘ğ‘¥(0, ğ‘¥)\n",
    "- solver ( {â€˜lbfgsâ€™, â€˜sgdâ€™, â€˜adamâ€™}, default=â€˜adamâ€™): regula de invatare (update)\n",
    "- â€˜sgdâ€™ - stochastic gradient descent (doar pe acesta il vom folosi).\n",
    "- batch_size: (int, default=â€˜autoâ€™)\n",
    "- auto - marimea batch-ului pentru antrenare este min(200, n_samples).\n",
    "- learning_rate_init (double, default=0.001): rata de invatare\n",
    "- max_iter (int, default=200): numarul maxim de epoci pentru antrenare.\n",
    "- shuffle (bool, default=True): amesteca datele la fiecare epoca\n",
    "- tol (float, default=1e-4) :\n",
    "- Daca eroarea sau scorul nu se imbunatatesc timp n_iter_no_chage\n",
    "epoci consecutive (si learning_rate != â€˜adaptiveâ€™) cu cel putin tol,\n",
    "antrenarea se opreste.\n",
    "- n_iter_no_change : (int, optional, default 10, sklearn-versiune-0.20)\n",
    "- Numarul maxim de epoci fara imbunatatiri (eroare sau scor).\n",
    "- alpha (float, default=0.0001): parametru pentru regularizare L2.\n",
    "- learning_rate ( {â€˜constantâ€™, â€˜invscalingâ€™, â€˜adaptiveâ€™}, default=â€˜constantâ€™ ):\n",
    "- â€˜constantâ€™ : rata de invatare este constanta si este data de parametrul\n",
    "learning_rate_init.\n",
    "- â€˜invscalingâ€™: rata de invatare va fi scazuta la fiecare pas t, dupa\n",
    "formula: new_learning_rate = learning_rate_init / pow(t, power_t)\n",
    "- â€˜adaptiveâ€™: pastreaza rata de invatare constanta cat timp eroarea\n",
    "scade. Daca eroarea nu scade cu cel putin tol (fata de epoca anterior)\n",
    "sau daca scorul pe multimea de validare (doar daca\n",
    "ealy_stopping=True) nu creste cu cel putin tol (fata de epoca\n",
    "anteriora), rata de invatare curenta se imparte la 5.\n",
    "- power_t (double, default=0.5): parametrul pentru learning_rate=â€™invscalingâ€™.\n",
    "- momentum (float, default=0.9): - valoarea pentru momentum cand se\n",
    "foloseste gradient descent cu momentum. Trebuie sa fie intre 0 si 1.\n",
    "- early_stopping (bool, default=False):\n",
    "- Daca este setat cu True atunci antrenarea se va termina daca eroarea\n",
    "pe multimea de validare nu se imbunatateste timp n_iter_no_chage\n",
    "epoci consecutive cu cel putin tol.\n",
    "- validation_fraction (float, optional, default=0.1):\n",
    "- Procentul din multimea de antrenare care sa fie folosit pentru validare\n",
    "(doar cand early_stopping=True). Trebuie sa fie intre 0 si 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3314f407",
   "metadata": {},
   "source": [
    "Mai departe in restul laboratorului ne vom focusa pe implementara retelelor neuronale folosind libraria Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a389f77",
   "metadata": {},
   "source": [
    "### Install Pytorch\n",
    "\n",
    "\n",
    "Accesati linkul: https://pytorch.org, iar la sectiunea \"Install Pytorch\" selectati detaliile conform specificatiilor masinii voastre. Mai precis, daca masina dispune de o placa video atunci lasati selectia nemodificata, in caz contrar selectati CPU in campul \"Compute Platform\".\n",
    "\n",
    "Exemplu configuratie masina cu GPU:\n",
    "\n",
    "![pytorch_gpu.png](./assets/pytorch_gpu.png)\n",
    "\n",
    "Exemplu configuratie masina doar cu CPU:\n",
    "\n",
    "![pytorch_cpu.png](./assets/pytorch_cpu.png)\n",
    "\n",
    "Pentru a verifica daca instalarea a fost cu succes, puteti rula urmatorul bloc de cod:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a886e425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1884, 0.4039, 0.3352],\n",
      "        [0.8100, 0.5215, 0.1551],\n",
      "        [0.1485, 0.1562, 0.8069],\n",
      "        [0.4286, 0.6763, 0.8859],\n",
      "        [0.1566, 0.0437, 0.3820]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a10b1",
   "metadata": {},
   "source": [
    "Pentru a verifica daca GPU-ul este accesibil de catre Pytorch, puteti rula codul urmator. Daca totul este in regula, ultima linie ar trebui sa returneze True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644ed1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c79ea",
   "metadata": {},
   "source": [
    "### Definirea retelei neuronale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e098ff23",
   "metadata": {},
   "source": [
    "Pentru a crea un model in Pytorch este necesar sa extindem clasa **nn.Module**, iar in constructor vom defini straturile retelei care vor fi folosite in implementarea functiei **forward**. Mai jos aveti un exemplu pentru un Multilayer Perceptron cu un singur strat ascuns.\n",
    "\n",
    "- stratul **Flatten** transforma datele de intrare in vectori 1-dimensionali.\n",
    "- stratul **Linear** aplica o transformare liniara: xW<sup>T</sup>+b. Pentru acest strat trebuie sa specificam dimensiunile matricei W, care corespund cu dimensiunea tensorilor de intrare si iesire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb817ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.first_layer = nn.Linear(28*28, 512)\n",
    "        self.second_layer = nn.Linear(512, 512)\n",
    "        self.output_layer = nn.Linear(512, 10)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.first_layer(x))\n",
    "        x = F.relu(self.second_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ee12d",
   "metadata": {},
   "source": [
    "Trecerea unui exemplu prin reteaua precedenta se poate executa in felul urmator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12088a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0728, -0.0668, -0.0012, -0.0073,  0.0726,  0.0236, -0.0149,  0.0356,\n",
       "         -0.0290,  0.0190],\n",
       "        [-0.0376, -0.0426,  0.0368, -0.0082,  0.0689,  0.0349, -0.0054,  0.0302,\n",
       "          0.0019, -0.0152],\n",
       "        [-0.0588, -0.0473,  0.0076,  0.0155,  0.0640,  0.0223, -0.0632,  0.0367,\n",
       "          0.0202,  0.0437],\n",
       "        [-0.0643, -0.0471,  0.0055,  0.0272,  0.0909,  0.0586,  0.0085,  0.0060,\n",
       "          0.0109,  0.0346],\n",
       "        [-0.0514, -0.0213,  0.0215,  0.0318,  0.0431,  0.0424, -0.0031,  0.0499,\n",
       "         -0.0246,  0.0240]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model(torch.rand(5, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cffd860",
   "metadata": {},
   "source": [
    "### Antrenarea retelei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74a382",
   "metadata": {},
   "source": [
    "Pentru antrenarea retelei avem nevoie de date de antrenare, un algoritm de optimizare si o functie de pierdere pe care sa o minimizam pe setul de antrenare.\n",
    "\n",
    "Vom folosi MNIST pentru a ilustra o procedura de antrenare in Pytorch, ca algoritm de optimizare vom folosi stochastic gradient descent (SGD), iar functia de optimizare va fi cross entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a44d27",
   "metadata": {},
   "source": [
    "Crearea seturilor de date si a dataloader-lor care ne vor ajuta sa iteram prin batch-uri in timpul unei epoci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7154e5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b4b5a68c5a4ffd8cdc1d95bda98a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b17d0323c243f1823831a594086690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8586d721c9b9486db885872c1fbf1efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee21b84224344b81af4d33c8a362e616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets \n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dfd515",
   "metadata": {},
   "source": [
    "Crearea modelului si definirea algoritmului de optimizare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f025847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb4453",
   "metadata": {},
   "source": [
    "Antrenarea retelei :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7254a795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1 ===\n",
      "Batch index 0, loss: 2.037801\n",
      "Batch index 100, loss: 1.764261\n",
      "Batch index 200, loss: 1.596179\n",
      "Batch index 300, loss: 1.110204\n",
      "Batch index 400, loss: 0.928878\n",
      "Batch index 500, loss: 0.762543\n",
      "Batch index 600, loss: 0.606124\n",
      "Batch index 700, loss: 0.733767\n",
      "Batch index 800, loss: 0.623447\n",
      "Batch index 900, loss: 0.554930\n",
      "=== Epoch 2 ===\n",
      "Batch index 0, loss: 0.595891\n",
      "Batch index 100, loss: 0.421773\n",
      "Batch index 200, loss: 0.449756\n",
      "Batch index 300, loss: 0.473317\n",
      "Batch index 400, loss: 0.400112\n",
      "Batch index 500, loss: 0.419373\n",
      "Batch index 600, loss: 0.277204\n",
      "Batch index 700, loss: 0.492012\n",
      "Batch index 800, loss: 0.432486\n",
      "Batch index 900, loss: 0.462994\n",
      "=== Epoch 3 ===\n",
      "Batch index 0, loss: 0.395996\n",
      "Batch index 100, loss: 0.300016\n",
      "Batch index 200, loss: 0.303191\n",
      "Batch index 300, loss: 0.402809\n",
      "Batch index 400, loss: 0.307577\n",
      "Batch index 500, loss: 0.363944\n",
      "Batch index 600, loss: 0.212678\n",
      "Batch index 700, loss: 0.426307\n",
      "Batch index 800, loss: 0.376609\n",
      "Batch index 900, loss: 0.433333\n",
      "=== Epoch 4 ===\n",
      "Batch index 0, loss: 0.311885\n",
      "Batch index 100, loss: 0.270453\n",
      "Batch index 200, loss: 0.241118\n",
      "Batch index 300, loss: 0.374615\n",
      "Batch index 400, loss: 0.264102\n",
      "Batch index 500, loss: 0.330615\n",
      "Batch index 600, loss: 0.186456\n",
      "Batch index 700, loss: 0.391804\n",
      "Batch index 800, loss: 0.337508\n",
      "Batch index 900, loss: 0.412028\n",
      "=== Epoch 5 ===\n",
      "Batch index 0, loss: 0.261263\n",
      "Batch index 100, loss: 0.257259\n",
      "Batch index 200, loss: 0.207579\n",
      "Batch index 300, loss: 0.355825\n",
      "Batch index 400, loss: 0.235868\n",
      "Batch index 500, loss: 0.307624\n",
      "Batch index 600, loss: 0.170920\n",
      "Batch index 700, loss: 0.368278\n",
      "Batch index 800, loss: 0.304966\n",
      "Batch index 900, loss: 0.391840\n",
      "=== Epoch 6 ===\n",
      "Batch index 0, loss: 0.225632\n",
      "Batch index 100, loss: 0.247253\n",
      "Batch index 200, loss: 0.185777\n",
      "Batch index 300, loss: 0.338797\n",
      "Batch index 400, loss: 0.214957\n",
      "Batch index 500, loss: 0.290452\n",
      "Batch index 600, loss: 0.158969\n",
      "Batch index 700, loss: 0.349359\n",
      "Batch index 800, loss: 0.276228\n",
      "Batch index 900, loss: 0.371843\n",
      "=== Epoch 7 ===\n",
      "Batch index 0, loss: 0.199742\n",
      "Batch index 100, loss: 0.237410\n",
      "Batch index 200, loss: 0.169984\n",
      "Batch index 300, loss: 0.322835\n",
      "Batch index 400, loss: 0.198558\n",
      "Batch index 500, loss: 0.277879\n",
      "Batch index 600, loss: 0.148789\n",
      "Batch index 700, loss: 0.332969\n",
      "Batch index 800, loss: 0.250050\n",
      "Batch index 900, loss: 0.353092\n",
      "=== Epoch 8 ===\n",
      "Batch index 0, loss: 0.179713\n",
      "Batch index 100, loss: 0.227393\n",
      "Batch index 200, loss: 0.157938\n",
      "Batch index 300, loss: 0.307701\n",
      "Batch index 400, loss: 0.184566\n",
      "Batch index 500, loss: 0.266270\n",
      "Batch index 600, loss: 0.139473\n",
      "Batch index 700, loss: 0.317934\n",
      "Batch index 800, loss: 0.226884\n",
      "Batch index 900, loss: 0.334073\n",
      "=== Epoch 9 ===\n",
      "Batch index 0, loss: 0.163646\n",
      "Batch index 100, loss: 0.217610\n",
      "Batch index 200, loss: 0.147357\n",
      "Batch index 300, loss: 0.293298\n",
      "Batch index 400, loss: 0.171481\n",
      "Batch index 500, loss: 0.258058\n",
      "Batch index 600, loss: 0.130816\n",
      "Batch index 700, loss: 0.302846\n",
      "Batch index 800, loss: 0.206051\n",
      "Batch index 900, loss: 0.315870\n",
      "=== Epoch 10 ===\n",
      "Batch index 0, loss: 0.150142\n",
      "Batch index 100, loss: 0.208507\n",
      "Batch index 200, loss: 0.137323\n",
      "Batch index 300, loss: 0.280230\n",
      "Batch index 400, loss: 0.159578\n",
      "Batch index 500, loss: 0.249143\n",
      "Batch index 600, loss: 0.121863\n",
      "Batch index 700, loss: 0.288235\n",
      "Batch index 800, loss: 0.188709\n",
      "Batch index 900, loss: 0.300472\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS=10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # decidem device-ul pe care sa il folosim cpu/cuda(gpu)\n",
    "model = model.to(device) \n",
    "loss_function = nn.CrossEntropyLoss() # functia ce trebuie optimizata, cross entropia\n",
    "\n",
    "model.train(True)\n",
    "for i in range(NUM_EPOCHS):\n",
    "    print(f\"=== Epoch {i+1} ===\")\n",
    "    for batch, (image_batch, labels_batch) in enumerate(train_dataloader): # iteram prin batch-uri\n",
    "        image_batch = image_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        \n",
    "        pred = model(image_batch) # procesam imaginile prin retea\n",
    "        loss = loss_function(pred, labels_batch) # determinam functia de pieredere folosind rezultatele retelei\n",
    "                                                 # si label-urile reale ale exemplelor de antrenare\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # optimizam parametrii retelei\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "                loss = loss.item()\n",
    "                print(f\"Batch index {batch }, loss: {loss:>7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04d0055",
   "metadata": {},
   "source": [
    "Testarea performantei:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee779b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.1%, Loss: 0.003190 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0.\n",
    "test_loss = 0.\n",
    "size = len(test_dataloader.dataset)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        for image_batch, labels_batch in test_dataloader: # iteram prin datele de test\n",
    "            \n",
    "            image_batch = image_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            pred = model(image_batch) # procesam imaginile folosind reteaua antrenata anterior\n",
    "            test_loss += loss_function(pred, labels_batch).item()\n",
    "            correct += (pred.argmax(1) == labels_batch).type(torch.float).sum().item() # numaram cate exemple sunt corect clasificate\n",
    "\n",
    "\n",
    "correct /= size\n",
    "test_loss /= size\n",
    "print(f\"Accuracy: {(100*correct):>0.1f}%, Loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bffe37",
   "metadata": {},
   "source": [
    "### Exercitii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c3cb2",
   "metadata": {},
   "source": [
    "1. Antrenati o retea de perceptroni care sa clasifice cifrele scrise de mana MNIST. Datele trebuie normalizate prin scaderea mediei si impartirea la deviatia standard. Antrenati pentru 5 epoci si testati urmatoarele configuratii de retele:\n",
    "\n",
    "a. Definiti o retea cu un singur strat ascuns cu un singur neuron si folositi ca functie de activare tanh. Pentru optimizator folositi un learning rate de 1e-2.\n",
    "\n",
    "b. Definiti o retea cu un singur strat ascuns cu 10 neuroni si folositi ca functie de activare tanh. Pentru optimizator folositi un learning rate de 1e-2.\n",
    "\n",
    "c. Definiti o retea cu un singur strat ascuns cu 10 neuroni si folositi ca functie de activare tanh. Pentru optimizator folositi un learning rate de 1e-5.\n",
    "\n",
    "d. Definiti o retea cu un singur strat ascuns cu 10 neuroni si folositi ca functie de activare tanh. Pentru optimizator folositi un learning rate de 10.\n",
    "\n",
    "e. Definiti o retea cu 2 straturi ascunse cu 10 neuroni fiecare si folositi ca functie de activare tanh. Pentru optimizator folositi un learning rate de 1e-2.\n",
    "\n",
    "f. Definiti o retea cu 2 straturi ascunse cu 10 neuroni fiecare si folositi ca functie de activare relu. Pentru optimizator folositi un learning rate de 1e-2.\n",
    "\n",
    "g. Definiti o retea cu 2 straturi ascunse cu 100 neuroni fiecare si folositi ca functie de activare relu. Pentru optimizator folositi un learning rate de 1e-2.\n",
    "\n",
    "h. Definiti o retea cu 2 straturi ascunse cu 100 neuroni fiecare si folositi ca functie de activare relu. Pentru optimizator folositi un learning rate de 1e-2 si momentum=0.9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
